---
title: "Homework 6"
author: "Shaolei Ma"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
library(latex2exp)
library(gridExtra)
library(modelr)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

Use the data cleaning procedure similar to HW5. Omit certain cities as instructed.

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = factor(
      case_when(
        disposition == "Closed without arrest" ~ "0",
        disposition == "Open/No arrest"        ~ "0",
        disposition == "Closed by arrest"      ~ "1"
    ),
    labels = c("unsolved", "solved"))
  ) |>  
  filter(
    city_state != "Tulsa, AL",
    city_state != "Dallas, TX",
    city_state != "Phoenix, AZ",
    city_state != "Kansas City, MO"
  )
```

The resulting dataframe has `r nrow(homicide_df)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. In cleaning, I created a `city_state` variable that includes both city and state, and a `resolution` variable to indicate whether the case was closed by arrest.

For the city of Baltimore, MD, use the `glm` function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors.

```{r}
baltimore_fit = 
  homicide_df |> 
  filter(city_state == "Baltimore, MD") |> 
  glm(resolution ~ victim_age + victim_sex + victim_race, data = _, family = binomial())

baltimore_fit |> 
  broom::tidy() |> 
  mutate(
    conf_low = estimate - 1.96 * std.error,
    conf_high = estimate + 1.96 * std.error,
    OR = exp(estimate),
    OR_conf_low = exp(conf_low),
    OR_conf_high = exp(conf_high)
  ) |> 
  filter(term == "victim_sexMale") |> 
  select(term, starts_with("OR"))
```

The adjusted odds ratio for solving homicides comparing male victims to female victims has an estimated value of 0.415 and a confidence interval [0.318, 0.542].

Now, run `glm` for each of the cities in the dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims.
```{r}
OR_sex_df =
  homicide_df |> 
  nest(df = - city_state) |> 
  mutate(
    models = map(df, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, data = df, family = binomial())),
    results = map(models, broom::tidy)
  ) |> 
  select(city_state, results) |> 
  unnest(results) |> 
  filter(term == "victim_sexMale") |>
  mutate(
    conf_low = estimate - 1.96 * std.error,
    conf_high = estimate + 1.96 * std.error,
    OR = exp(estimate),
    OR_conf_low = exp(conf_low),
    OR_conf_high = exp(conf_high)
  ) |> 
  select(city_state, term, starts_with("OR")) |> 
  arrange(OR)

OR_sex_df |> 
  knitr::kable()
```

Create a plot that shows the estimated ORs and CIs for each city. 
```{r}
OR_sex_df |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = OR, y = city_state)) +
  geom_point() +
  geom_errorbar(aes(xmin = OR_conf_low, xmax = OR_conf_high)) +
  labs(title = "OR Among Cities")
```

It could be concluded that New York, NY has the lowest OR, while Albuquerque, NM has the highest OR in terms of solving homicides comparing male victims to female victims.

# Problem 2

First, download the data.
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

Focus on a linear regression with `tmax` as the response with `tmin` and `prcp` as the predictors. Use 5000 bootstrap samples to produce estimates for $\hat r^2$ and $log(\hat \beta_1*\hat \beta_2)$.

```{r}
boot_sample = function(df) {
  
  sample_frac(df, replace = T)
  
}

boot_straps =
  tibble(strap_number = 1:5000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(weather_df))
  )

boot_results =
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::tidy)
  ) |> 
  unnest(results) |> 
  select(strap_number, models, term, estimate) |> 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) |> 
  mutate(
    log_beta12 = log(tmin * prcp),
    results = map(models, broom::glance)
  ) |> 
  unnest(results) |> 
  select(strap_number, r.squared, log_beta12) |> 
  pivot_longer(
    r.squared:log_beta12,
    names_to = "term",
    values_to = "estimate"
  )

p1 = 
  boot_results |> 
  filter(term == "r.squared") |> 
  ggplot(aes(x = estimate)) +
  geom_density() +
  labs(title = TeX("$\\hat{r^2}$ Estimate Distribution"),
       x = TeX("$\\hat{r^2}$"))

p2 = 
  boot_results |> 
  filter(term == "log_beta12") |> 
  ggplot(aes(x = estimate)) +
  geom_density() +
  labs(title = TeX("$log(\\hat{\\beta_1}*\\hat{\\beta_2})$ Estimate Distribution"),
       x = TeX("$log(\\hat{\\beta_1}*\\hat{\\beta_2})$"))

grid.arrange(p1, p2, nrow = 1)
```

The two distributions are both left skewed, which indicates that the variability observed in the `tmax` could be well explained by `tmin` and `prcp`, and the interaction between two independent variables should be considered. The mode for $\hat r_2$ is around 0.92, and the mode for $log(\hat \beta_1*\hat \beta_2)$ is around -5.5.
```{r echo=F}
tibble(
  r_square_low = boot_results |> filter(term == "r.squared") |> pull(estimate) |> quantile(0.025),
  r_square_high = boot_results |> filter(term == "r.squared") |> pull(estimate) |> quantile(0.975),
  log_beta_low = boot_results |> filter(term == "log_beta12") |> pull(estimate) |> quantile(0.025, na.rm = T),
  log_beta_high = boot_results |> filter(term == "log_beta12") |> pull(estimate) |> quantile(0.975, na.rm = T)
) |> 
  knitr::kable()
```

So the confidence interval for $\hat r_2$ is [0.89,0.94], and the confidence interval for $log(\hat \beta_1*\hat \beta_2)$ is [-9.13,-4.58].

# Problem 3
```{r}
birthweight_df =
  read_csv("data/birthweight.csv") |> 
  janitor::clean_names() |> 
  mutate( # convert numbers to factors
    babysex = factor(
      case_match(
        babysex,
        1 ~ "male",
        2 ~ "female"
      )
    ),
    frace = factor(
      case_match(
        frace,
        1 ~ "White",
        2 ~ "Black",
        3 ~ "Asian",
        4 ~ "Puerto Rican",
        8 ~ "Other",
        9 ~ "Unknown"
      )
    ),
    mrace = factor(
      case_match(
        mrace,
        1 ~ "White",
        2 ~ "Black",
        3 ~ "Asian",
        4 ~ "Puerto Rican",
        8 ~ "Other"
      )
    ),
    malform = factor(
      case_match(
        malform,
        0 ~ "absent",
        1 ~ "present"
      )
    )
  )
```

The resulting data set has `r nrow(birthweight_df)` observations and `r ncol(birthweight_df)` variables. There is no missing data.

Now, propose a regression model for birthweight. To decide on the independent variables, I follow the conclusion of a highly cited paper published in 1987, [Determinants of low birth weight: methodological assessment and meta-analysis.](https://pubmed.ncbi.nlm.nih.gov/3322602/) It mentioned that *"In developed countries, the most important factor was cigarette smoking, followed by nutrition and pre-pregnancy weight. In developing countries the major determinants were racial origin, nutrition, low pre-pregnancy weight, short maternal stature, and malaria."* Therefore, I choose `ppwt`, `frace`, `mrace`, `mheight`, `smoken` as the independent variables.
```{r}
birthweight_fit =
  birthweight_df |> 
  lm(bwt ~ ppwt + frace + mrace + mheight + smoken, data = _)

birthweight_df |> 
  modelr::add_predictions(birthweight_fit) |> 
  modelr::add_residuals(birthweight_fit) |> 
  ggplot(aes(x = pred, y = resid)) +
  geom_point()
```

From the scatterplot, it could be concluded that there is no relevance between the residuals and fitted values.

Compare the model to two others:

 * First: One using length at birth and gestational age as predictors (main effects only)

 * Second: One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
 
```{r}
cv_df = 
  crossv_mc(birthweight_df, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) |> 
  mutate(
    mod0  = map(train, \(df) lm(bwt ~ ppwt + frace + mrace + mheight + smoken, data = df)),
    mod1  = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    mod2  = map(train, \(df) lm(bwt ~ bhead + blength + babysex + bhead * blength + blength * babysex + bhead * babysex + bhead * blength * babysex, data = df))) |> 
  mutate(
    rmse0 = map2_dbl(mod0, test, \(mod, df) rmse(model = mod, data = df)),
    rmse1 = map2_dbl(mod1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse2 = map2_dbl(mod2, test, \(mod, df) rmse(model = mod, data = df))
  )

cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

According to the plots shown above, it could be concluded that both two models perform better than the proposed model, and the second model has the lowest RMSE.