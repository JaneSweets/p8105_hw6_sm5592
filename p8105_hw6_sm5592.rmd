---
title: "Homework 6"
author: "Shaolei Ma"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
library(latex2exp)
library(gridExtra)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

Use the data cleaning procedure similar to HW5. Omit certain cities as instructed.

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = factor(
      case_when(
        disposition == "Closed without arrest" ~ "0",
        disposition == "Open/No arrest"        ~ "0",
        disposition == "Closed by arrest"      ~ "1"
    ),
    labels = c("unsolved", "solved"))
  ) |>  
  filter(
    city_state != "Tulsa, AL",
    city_state != "Dallas, TX",
    city_state != "Phoenix, AZ",
    city_state != "Kansas City, MO"
  )
```

The resulting dataframe has `r nrow(homicide_df)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. In cleaning, I created a `city_state` variable that includes both city and state, and a `resolution` variable to indicate whether the case was closed by arrest.

For the city of Baltimore, MD, use the `glm` function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors.

```{r}
baltimore_fit = 
  homicide_df |> 
  filter(city_state == "Baltimore, MD") |> 
  glm(resolution ~ victim_age + victim_sex + victim_race, data = _, family = binomial())

baltimore_fit |> 
  broom::tidy() |> 
  mutate(
    conf_low = estimate - 1.96 * std.error,
    conf_high = estimate + 1.96 * std.error,
    OR = exp(estimate),
    OR_conf_low = exp(conf_low),
    OR_conf_high = exp(conf_high)
  ) |> 
  filter(term == "victim_sexMale") |> 
  select(term, starts_with("OR"))
```

The adjusted odds ratio for solving homicides comparing male victims to female victims has an estimated value of 0.415 and a confidence interval [0.318, 0.542].

Now, run `glm` for each of the cities in the dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims.
```{r}
OR_sex_df =
  homicide_df |> 
  nest(df = - city_state) |> 
  mutate(
    models = map(df, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, data = df, family = binomial())),
    results = map(models, broom::tidy)
  ) |> 
  select(city_state, results) |> 
  unnest(results) |> 
  filter(term == "victim_sexMale") |>
  mutate(
    conf_low = estimate - 1.96 * std.error,
    conf_high = estimate + 1.96 * std.error,
    OR = exp(estimate),
    OR_conf_low = exp(conf_low),
    OR_conf_high = exp(conf_high)
  ) |> 
  select(city_state, term, starts_with("OR")) |> 
  arrange(OR)

OR_sex_df |> 
  knitr::kable()
```

Create a plot that shows the estimated ORs and CIs for each city. 
```{r}
OR_sex_df |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = OR, y = city_state)) +
  geom_point() +
  geom_errorbar(aes(xmin = OR_conf_low, xmax = OR_conf_high)) +
  labs(title = "OR Among Cities")
```

It could be concluded that New York, NY has the lowest OR, while Albuquerque, NM has the highest OR in terms of solving homicides comparing male victims to female victims.

# Problem 2

First, download the data.
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

Focus on a linear regression with `tmax` as the response with `tmin` and `prcp` as the predictors. Use 5000 bootstrap samples to produce estimates for $\hat r^2$ and $log(\hat \beta_1*\hat \beta_2)$.

```{r}
boot_sample = function(df) {
  
  sample_frac(df, replace = T)
  
}

boot_straps =
  tibble(strap_number = 1:5000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(weather_df))
  )

boot_results =
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::tidy)
  ) |> 
  unnest(results) |> 
  select(strap_number, models, term, estimate) |> 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) |> 
  mutate(
    log_beta12 = log(tmin * prcp),
    results = map(models, broom::glance)
  ) |> 
  unnest(results) |> 
  select(strap_number, r.squared, log_beta12) |> 
  pivot_longer(
    r.squared:log_beta12,
    names_to = "term",
    values_to = "estimate"
  )

p1 = 
  boot_results |> 
  filter(term == "r.squared") |> 
  ggplot(aes(x = estimate)) +
  geom_density() +
  labs(title = TeX("$\\hat{r^2}$ Estimate Distribution"),
       x = TeX("$\\hat{r^2}$"))

p2 = 
  boot_results |> 
  filter(term == "log_beta12") |> 
  ggplot(aes(x = estimate)) +
  geom_density() +
  labs(title = TeX("$log(\\hat{\\beta_1}*\\hat{\\beta_2})$ Estimate Distribution"),
       x = TeX("$log(\\hat{\\beta_1}*\\hat{\\beta_2})$"))

grid.arrange(p1, p2, nrow = 1)
```

The two distributions are both left skewed, which indicates that the variability observed in the `tmax` could be well explained by `tmin` and `prcp`, and the interaction between two independent variables should be considered. The mode for $\hat r_2$ is around 0.92, and the mode for $log(\hat \beta_1*\hat \beta_2)$ is around -5.5.
```{r echo=F}
tibble(
  r_square_low = boot_results |> filter(term == "r.squared") |> pull(estimate) |> quantile(0.025),
  r_square_high = boot_results |> filter(term == "r.squared") |> pull(estimate) |> quantile(0.975),
  log_beta_low = boot_results |> filter(term == "log_beta12") |> pull(estimate) |> quantile(0.025, na.rm = T),
  log_beta_high = boot_results |> filter(term == "log_beta12") |> pull(estimate) |> quantile(0.975, na.rm = T)
) |> 
  knitr::kable()
```

So the confidence interval for $\hat r_2$ is [0.89,0.94], and the confidence interval for $log(\hat \beta_1*\hat \beta_2)$ is [-9.13,-4.58].

